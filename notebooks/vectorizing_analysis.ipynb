{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comics = pd.read_csv('../data/cleaned_data/comics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = pd.read_csv('../data/cleaned_data/zone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delibrately used each dataset separately to showcase the frequency of various lemma in each separate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "author          0\n",
       "subreddit       0\n",
       "lems          171\n",
       "title_lems      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are some null values in both dataframes most likely from the deletion of the 'remove' lemma \n",
    "comics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0      0\n",
       "author          0\n",
       "subreddit       0\n",
       "lems          121\n",
       "title_lems      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on my subject knowledge; imputed NaN with common terms \n",
    "comics.fillna(value='comic', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone.fillna(value='twilight', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "author        0\n",
       "subreddit     0\n",
       "lems          0\n",
       "title_lems    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "author        0\n",
       "subreddit     0\n",
       "lems          0\n",
       "title_lems    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Comparing Twlight Zone Phrases with Comic Book Phrases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with a longer n_gram range (3-5 words) for Twilight Zone and Comic Books\n",
    "cvec_comic_phrase = CountVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')\n",
    "tvec_comic_phrase = TfidfVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')\n",
    "cvec_zone_phrase = CountVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')\n",
    "tvec_zone_phrase = TfidfVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_comics = cvec_comic_phrase.fit_transform(comics['lems'])\n",
    "tvec_comics = tvec_comic_phrase.fit_transform(comics['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_zone = cvec_zone_phrase.fit_transform(zone['lems'])\n",
    "tvec_zone = tvec_zone_phrase.fit_transform(zone['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Comics\n",
    "cvec_comics_df = pd.DataFrame(cvec_comics.toarray(), columns=cvec_comic_phrase.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_comics_df = pd.DataFrame(tvec_comics.toarray(), columns=tvec_comic_phrase.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Twilight Zone\n",
    "cvec_zone_df = pd.DataFrame(cvec_zone.toarray(), columns=cvec_zone_phrase.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_zone_df = pd.DataFrame(tvec_zone.toarray(), columns=tvec_zone_phrase.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twilight zone episode         109\n",
       "http audioboom com             54\n",
       "wikipedia org wiki             51\n",
       "en wikipedia org               51\n",
       "http en wikipedia              51\n",
       "http en wikipedia org          51\n",
       "http en wikipedia org wiki     51\n",
       "en wikipedia org wiki          51\n",
       "episode info http en           40\n",
       "imdb opening narration         40\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "cvec_counts = cvec_zone_df.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twilight zone episode         16.840045\n",
       "new twilight zone              7.714732\n",
       "episode twilight zone          7.155298\n",
       "like twilight zone             6.525209\n",
       "original twilight zone         6.007025\n",
       "http en wikipedia org wiki     5.951970\n",
       "http en wikipedia              5.951970\n",
       "http en wikipedia org          5.951970\n",
       "en wikipedia org wiki          5.951970\n",
       "en wikipedia org               5.951970\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "tvec_counts = tvec_zone_df.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key phrases in The Twilight Zone texts should prove useful for categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reddit com comicbooks                238\n",
       "www reddit com comicbooks            237\n",
       "http www reddit                      237\n",
       "www reddit com                       237\n",
       "http www reddit com comicbooks       237\n",
       "http www reddit com                  237\n",
       "com comicbooks comment               230\n",
       "reddit com comicbooks comment        230\n",
       "www reddit com comicbooks comment    229\n",
       "utm_source share amp utm_medium      189\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "cvec_counts = cvec_comics_df.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "amazing spider man                8.365159\n",
       "http imgur com                    7.881666\n",
       "amp auto webp                     6.431498\n",
       "http preview redd                 6.431498\n",
       "amp auto webp amp                 6.431498\n",
       "auto webp amp                     6.431498\n",
       "reddit com comicbooks             5.462987\n",
       "www reddit com comicbooks         5.428264\n",
       "http www reddit com comicbooks    5.428264\n",
       "http www reddit com               5.428264\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "tvec_counts = tvec_comics_df.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key phrases in the Comic Book texts appear to mostly be https code; however, both Ultimate Spider Man and Cosmic Ghost Rider are solid classification leads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Comparing Twilight Zone Words and Comic Book Words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with a shorter n_gram range (1-2 words) for Twilight Zone and Comic Books\n",
    "cvec_comic_word = CountVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')\n",
    "tvec_comic_word = TfidfVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')\n",
    "cvec_zone_word = CountVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')\n",
    "tvec_zone_word = TfidfVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_comics_words = cvec_comic_word.fit_transform(comics['lems'])\n",
    "tvec_comics_words = tvec_comic_word.fit_transform(comics['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_zone_words = cvec_zone_word.fit_transform(zone['lems'])\n",
    "tvec_zone_words = tvec_zone_word.fit_transform(zone['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Comics\n",
    "cvec_comics_df_words = pd.DataFrame(cvec_comics_words.toarray(), columns=cvec_comic_word.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_comics_df_words = pd.DataFrame(tvec_comics_words.toarray(), columns=tvec_comic_word.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Twilight Zone\n",
    "cvec_zone_df_words = pd.DataFrame(cvec_zone_words.toarray(), columns=cvec_zone_word.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_zone_df_words = pd.DataFrame(tvec_zone_words.toarray(), columns=tvec_zone_word.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "episode          2000\n",
       "wa               1068\n",
       "like              787\n",
       "just              782\n",
       "twilight          743\n",
       "zone              698\n",
       "twilight zone     613\n",
       "time              475\n",
       "think             435\n",
       "ha                405\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "cvec_counts = cvec_zone_df_words.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twilight         148.221073\n",
       "episode           62.118515\n",
       "wa                41.703851\n",
       "zone              33.029435\n",
       "just              31.630621\n",
       "like              30.307026\n",
       "twilight zone     29.822896\n",
       "know              22.290853\n",
       "think             22.189892\n",
       "time              22.006632\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "tvec_counts = tvec_zone_df_words.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keywords such as 'episode' and 'twilight' should prove useful as classification predictors. Also, models should explicitly state an n_gram range of only one or two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comic    1434\n",
       "wa        700\n",
       "amp       695\n",
       "http      691\n",
       "like      644\n",
       "book      624\n",
       "just      542\n",
       "com       530\n",
       "read      511\n",
       "know      422\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "cvec_counts = cvec_comics_df_words.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comic      228.656371\n",
       "removed     99.067142\n",
       "wa          43.282156\n",
       "read        41.382454\n",
       "like        41.092841\n",
       "book        37.686402\n",
       "just        36.966652\n",
       "know        33.891183\n",
       "amp         32.668809\n",
       "story       29.320749\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "tvec_counts = tvec_comics_df_words.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keywords such as 'comic' and 'wa' should prove useful as classification predictors. Also, models should explicitly state an n_gram range of only one or two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
