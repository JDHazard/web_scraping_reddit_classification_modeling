{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 5_000\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "comics = pd.read_csv('../data/cleaned_data/comics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone = pd.read_csv('../data/cleaned_data/zone')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Delibrately used each dataset separately to showcase the frequency of various lemma in each separate dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "author         0\n",
       "subreddit      0\n",
       "lems          78\n",
       "title_lems     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "author         0\n",
       "subreddit      0\n",
       "lems          78\n",
       "title_lems     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# There are some null values in both dataframes most likely from the deletion of the 'remove' lemma \n",
    "comics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "author         0\n",
       "subreddit      0\n",
       "lems          54\n",
       "title_lems     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     0\n",
       "author         0\n",
       "subreddit      0\n",
       "lems          54\n",
       "title_lems     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on my subject knowledge; imputed NaN with common terms \n",
    "comics.fillna(value='comic', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone.fillna(value='twilight', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "author        0\n",
       "subreddit     0\n",
       "lems          0\n",
       "title_lems    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "author        0\n",
       "subreddit     0\n",
       "lems          0\n",
       "title_lems    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comics.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "author        0\n",
       "subreddit     0\n",
       "lems          0\n",
       "title_lems    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Unnamed: 0    0\n",
       "author        0\n",
       "subreddit     0\n",
       "lems          0\n",
       "title_lems    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zone.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Comparing Twlight Zone Phrases with Comic Book Phrases_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with a longer n_gram range (3-5 words) for Twilight Zone and Comic Books\n",
    "cvec_comic_phrase = CountVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')\n",
    "tvec_comic_phrase = TfidfVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')\n",
    "cvec_zone_phrase = CountVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')\n",
    "tvec_zone_phrase = TfidfVectorizer(ngram_range= [3,5], max_features = 10_000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_comics = cvec_comic_phrase.fit_transform(comics['lems'])\n",
    "tvec_comics = tvec_comic_phrase.fit_transform(comics['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_zone = cvec_zone_phrase.fit_transform(zone['lems'])\n",
    "tvec_zone = tvec_zone_phrase.fit_transform(zone['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Comics\n",
    "cvec_comics_df = pd.DataFrame(cvec_comics.toarray(), columns=cvec_comic_phrase.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_comics_df = pd.DataFrame(tvec_comics.toarray(), columns=tvec_comic_phrase.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Twilight Zone\n",
    "cvec_zone_df = pd.DataFrame(cvec_zone.toarray(), columns=cvec_zone_phrase.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_zone_df = pd.DataFrame(tvec_zone.toarray(), columns=tvec_zone_phrase.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twilight zone episode         18\n",
       "new twilight zone             11\n",
       "nightmare 30 000               9\n",
       "en wikipedia org               7\n",
       "http en wikipedia org wiki     7\n",
       "en wikipedia org wiki          7\n",
       "30 000 feet                    7\n",
       "wikipedia org wiki             7\n",
       "like twilight zone             7\n",
       "http en wikipedia org          7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "twilight zone episode         18\n",
       "new twilight zone             11\n",
       "nightmare 30 000               9\n",
       "en wikipedia org               7\n",
       "http en wikipedia org wiki     7\n",
       "en wikipedia org wiki          7\n",
       "30 000 feet                    7\n",
       "wikipedia org wiki             7\n",
       "like twilight zone             7\n",
       "http en wikipedia org          7\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "cvec_counts = cvec_zone_df.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "new twilight zone        2.876261\n",
       "twilight zone episode    2.215771\n",
       "nightmare 30 000         1.770906\n",
       "30 000 foot              1.496494\n",
       "shot black white         1.492005\n",
       "does know episode        1.475496\n",
       "time new episode         1.429422\n",
       "really want watch        1.335776\n",
       "twilight zone 2019       1.309941\n",
       "amp x200b http           1.192770\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "new twilight zone        2.876261\n",
       "twilight zone episode    2.215771\n",
       "nightmare 30 000         1.770906\n",
       "30 000 foot              1.496494\n",
       "shot black white         1.492005\n",
       "does know episode        1.475496\n",
       "time new episode         1.429422\n",
       "really want watch        1.335776\n",
       "twilight zone 2019       1.309941\n",
       "amp x200b http           1.192770\n",
       "dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "tvec_counts = tvec_zone_df.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key phrases in The Twilight Zone texts should prove useful for categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "www reddit com                       91\n",
       "http www reddit com                  91\n",
       "http www reddit                      91\n",
       "http www reddit com comicbooks       88\n",
       "www reddit com comicbooks            88\n",
       "reddit com comicbooks                88\n",
       "com comicbooks comment               85\n",
       "reddit com comicbooks comment        85\n",
       "www reddit com comicbooks comment    85\n",
       "http twitter com                     30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "www reddit com                       91\n",
       "http www reddit com                  91\n",
       "http www reddit                      91\n",
       "http www reddit com comicbooks       88\n",
       "www reddit com comicbooks            88\n",
       "reddit com comicbooks                88\n",
       "com comicbooks comment               85\n",
       "reddit com comicbooks comment        85\n",
       "www reddit com comicbooks comment    85\n",
       "http twitter com                     30\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "cvec_counts = cvec_comics_df.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ultimate spider man    4.372686\n",
       "amp x200b http         2.600831\n",
       "x200b http redd        2.534287\n",
       "amp x200b http redd    2.534287\n",
       "comic book store       2.450544\n",
       "http imgur com         2.173068\n",
       "like title say         2.165949\n",
       "cosmic ghost rider     2.153964\n",
       "amp x200b amp x200b    2.023190\n",
       "amp x200b amp          2.023190\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "ultimate spider man    4.372686\n",
       "amp x200b http         2.600831\n",
       "x200b http redd        2.534287\n",
       "amp x200b http redd    2.534287\n",
       "comic book store       2.450544\n",
       "http imgur com         2.173068\n",
       "like title say         2.165949\n",
       "cosmic ghost rider     2.153964\n",
       "amp x200b amp x200b    2.023190\n",
       "amp x200b amp          2.023190\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "tvec_counts = tvec_comics_df.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Key phrases in the Comic Book texts appear to mostly be https code; however, both Ultimate Spider Man and Cosmic Ghost Rider are solid classification leads. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Comparing Twilight Zone Words and Comic Book Words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a vectorizer with a shorter n_gram range (1-2 words) for Twilight Zone and Comic Books\n",
    "cvec_comic_word = CountVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')\n",
    "tvec_comic_word = TfidfVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')\n",
    "cvec_zone_word = CountVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')\n",
    "tvec_zone_word = TfidfVectorizer(ngram_range= [1, 2], max_features = 5000, stop_words = 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_comics_words = cvec_comic_word.fit_transform(comics['lems'])\n",
    "tvec_comics_words = tvec_comic_word.fit_transform(comics['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use fit_transform function on the 'lems' text column \n",
    "cvec_zone_words = cvec_zone_word.fit_transform(zone['lems'])\n",
    "tvec_zone_words = tvec_zone_word.fit_transform(zone['lems'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Comics\n",
    "cvec_comics_df_words = pd.DataFrame(cvec_comics_words.toarray(), columns=cvec_comic_word.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_comics_df_words = pd.DataFrame(tvec_comics_words.toarray(), columns=tvec_comic_word.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a df for vectorized words; Twilight Zone\n",
    "cvec_zone_df_words = pd.DataFrame(cvec_zone_words.toarray(), columns=cvec_zone_word.get_feature_names())\n",
    "# creating a df for tfidf words\n",
    "tvec_zone_df_words = pd.DataFrame(tvec_zone_words.toarray(), columns=tvec_zone_word.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "episode          427\n",
       "wa               291\n",
       "just             231\n",
       "twilight         210\n",
       "like             199\n",
       "10               192\n",
       "zone             161\n",
       "twilight zone    153\n",
       "think            131\n",
       "people           128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "episode          427\n",
       "wa               291\n",
       "just             231\n",
       "twilight         210\n",
       "like             199\n",
       "10               192\n",
       "zone             161\n",
       "twilight zone    153\n",
       "think            131\n",
       "people           128\n",
       "dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "cvec_counts = cvec_zone_df_words.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "twilight         62.093802\n",
       "episode          18.964003\n",
       "wa               13.938632\n",
       "10               13.907478\n",
       "just             11.019301\n",
       "zone             10.034104\n",
       "twilight zone     9.771264\n",
       "like              8.955099\n",
       "know              8.130613\n",
       "think             7.866082\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "twilight         62.093802\n",
       "episode          18.964003\n",
       "wa               13.938632\n",
       "10               13.907478\n",
       "just             11.019301\n",
       "zone             10.034104\n",
       "twilight zone     9.771264\n",
       "like              8.955099\n",
       "know              8.130613\n",
       "think             7.866082\n",
       "dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Twilight Zone\n",
    "tvec_counts = tvec_zone_df_words.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keywords such as 'episode' and 'twilight' should prove useful as classification predictors. Also, models should explicitly state an n_gram range of only one or two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comic    780\n",
       "wa       490\n",
       "like     421\n",
       "http     349\n",
       "book     342\n",
       "just     322\n",
       "read     305\n",
       "com      282\n",
       "amp      252\n",
       "know     226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "comic    780\n",
       "wa       490\n",
       "like     421\n",
       "http     349\n",
       "book     342\n",
       "just     322\n",
       "read     305\n",
       "com      282\n",
       "amp      252\n",
       "know     226\n",
       "dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "cvec_counts = cvec_comics_df_words.sum(axis=0)\n",
    "cvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comic        108.422742\n",
       "wa            25.970053\n",
       "like          23.110696\n",
       "read          22.801493\n",
       "just          20.668383\n",
       "book          20.468026\n",
       "removed       17.405006\n",
       "know          16.655738\n",
       "character     14.473864\n",
       "story         14.345642\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "comic        108.422742\n",
       "wa            25.970053\n",
       "like          23.110696\n",
       "read          22.801493\n",
       "just          20.668383\n",
       "book          20.468026\n",
       "removed       17.405006\n",
       "know          16.655738\n",
       "character     14.473864\n",
       "story         14.345642\n",
       "dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looking at vectorized value counts; Comics\n",
    "tvec_counts = tvec_comics_df_words.sum(axis=0)\n",
    "tvec_counts.sort_values(ascending=False)[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Keywords such as 'comic' and 'wa' should prove useful as classification predictors. Also, models should explicitly state an n_gram range of only one or two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
